{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import libs.resources as r\n",
    "import libs.pipeline as pipe\n",
    "import libs.files as fh\n",
    "import numpy as np\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 9683 rows from ../1-Input/trainingData-B.tsv\n",
      "Read 1653 rows from ../1-Input/devData-B.tsv\n",
      "Read 3812 rows from ../1-Input/testData-B.tsv\n"
     ]
    }
   ],
   "source": [
    "# read the information\n",
    "OUTPUT_DIR = '../3-Output/'\n",
    "MODEL_OUT_DIR = '../6-Models'\n",
    "CREATE_TOKENS_FILES = True\n",
    "PROCESS_DIR = '../2-Processed'\n",
    "\n",
    "# read the config file\n",
    "cfg = fh.read_config_file(\"all.yaml\")\n",
    "\n",
    "# read the train file\n",
    "file_name = \"../1-Input/trainingData-B.tsv\"\n",
    "file_type = 'B'\n",
    "train_feat, labels, train_tweets = pipe.create_features(file_name, file_type, cfg)\n",
    "\n",
    "# read the dev file\n",
    "file_name = \"../1-Input/devData-B.tsv\"\n",
    "file_type = 'B'\n",
    "dev_feat, dev_labels, dev_tweets = pipe.create_features(file_name, file_type, cfg)\n",
    "\n",
    "# read the test file\n",
    "file_name = \"../1-Input/testData-B.tsv\"\n",
    "file_type = 'B'\n",
    "test_feat, gold, test_tweets = pipe.create_features(file_name, file_type, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (8171, 18094)\n",
      "dev shape: (1405, 18094)\n",
      "test shape: (3239, 18094)\n"
     ]
    }
   ],
   "source": [
    "# create the featues for each lexicon\n",
    "# random seed for all the operations\n",
    "rnd_seed = 9000\n",
    "\n",
    "# convert the list into and array that can be indexed\n",
    "labels = np.array(labels)\n",
    "dev_labels = np.array(dev_labels)\n",
    "gold = np.array(gold)\n",
    "\n",
    "# create the cleaned tweets\n",
    "train_clean, dev_clean, vect = pipe.create_count_vec(train_tweets, dev_tweets, tokenizer=pipe.tokenize_clean_raw, stop_words=pipe.stop_words)\n",
    "test_clean = vect.transform(test_tweets)\n",
    "print \"test shape:\", test_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape (8171, 927) (1405, 927)\n",
      "Test Final Shape: (3239, 927)\n"
     ]
    }
   ],
   "source": [
    "# Auto select the best features\n",
    "train_data, dev_data, selec = pipe.auto_select_features(pipe.chi2, 5, train_clean, labels, dev_clean, dev_labels)\n",
    "test_data = selec.transform(test_clean)\n",
    "print('Test Final Shape: {}'.format(test_data.shape))\n",
    "\n",
    "# save the base features\n",
    "pipe.dump_data(train_data, 'train_base_data.pck')\n",
    "pipe.dump_data(dev_data, 'dev_base_data.pck')\n",
    "pipe.dump_data(test_data, 'test_base_data.pck')\n",
    "pipe.dump_data(labels, 'labels.pck')\n",
    "pipe.dump_data(dev_labels, 'dev_labels.pck')\n",
    "pipe.dump_data(gold, 'gold.pck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using negated tokens\n",
    "train_neg_tokens = [pipe.tokenize_negate_clean_raw(t) for t in train_tweets]\n",
    "dev_neg_tokens = [pipe.tokenize_negate_clean_raw(t) for t in dev_tweets]\n",
    "test_neg_tokens = [pipe.tokenize_negate_clean_raw(t) for t in test_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# not using negated tokens\n",
    "train_tokens = [pipe.tokenize_clean_raw(t) for t in train_tweets]\n",
    "dev_tokens = [pipe.tokenize_clean_raw(t) for t in dev_tweets]\n",
    "test_tokens = [pipe.tokenize_clean_raw(t) for t in test_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, mutual_info_classif\n",
    "\n",
    "def select_features(lex, train_feat, labels, dev_feat, test_feat):\n",
    "    \"\"\"\n",
    "    Select the most important features according to a given criteria.\n",
    "    \"\"\"\n",
    "    tmp_train_feat, tmp_dev_feat, tmp_test_feat, _ = pipe.create_lex_vec(train_feat, dev_feat, test_feat)\n",
    "    tmp_train_feat, tmp_dev_feat, selector = pipe.auto_select_features(mutual_info_classif, lex.selection_percent, \n",
    "                                                                       tmp_train_feat, labels, \n",
    "                                                                       tmp_dev_feat, None)\n",
    "    tmp_test_feat = selector.transform(tmp_test_feat)\n",
    "    return tmp_train_feat, tmp_dev_feat, tmp_test_feat\n",
    "    \n",
    "# generate each lexicon best features for the 3 datasets\n",
    "def save_features(lex, train_tokens, labels, dev_tokens, dev_labels, test_tokens):\n",
    "    \"\"\"\n",
    "    Generate the best features for each of the lexicons\n",
    "    \"\"\"\n",
    "    # create the features from the datasets tokens\n",
    "    if not lex.negated:\n",
    "        train_feat = lex.process_lex(train_tokens, use_best_features=True)\n",
    "        dev_feat = lex.process_lex(dev_tokens, use_best_features=True)\n",
    "        test_feat = lex.process_lex(test_tokens, use_best_features=True)\n",
    "    else:\n",
    "        train_feat = lex.process_lex(train_neg_tokens, use_best_features=True)\n",
    "        dev_feat = lex.process_lex(dev_neg_tokens, use_best_features=True)\n",
    "        test_feat = lex.process_lex(test_neg_tokens, use_best_features=True)\n",
    "    \n",
    "    # select the top percent features if this is the case. Set the flag to create vector at save if selection occurs\n",
    "    if lex.selection_percent:\n",
    "        train_feat, dev_feat, test_feat = select_features(lex, train_feat, labels, dev_feat, test_feat)\n",
    "    else:\n",
    "        train_feat, dev_feat, test_feat,_ = pipe.create_lex_vec(train_feat, dev_feat, test_feat)\n",
    "                \n",
    "    # dump the info\n",
    "    print 'saving {}\\t{}\\t{}'.format(train_feat.shape, dev_feat.shape, test_feat.shape)\n",
    "    pipe.dump_lex_features(lex, train_feat, dev_feat, create_vec=False, test=test_feat)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing WNA\n",
      "Final shape (8171, 91) (1405, 91)\n",
      "saving (8171, 91)\t(1405, 91)\t(3239, 91)\n",
      "Processing TSLEX\n",
      "saving (8171, 7)\t(1405, 7)\t(3239, 7)\n",
      "Processing SENT140\n",
      "saving (8171, 7)\t(1405, 7)\t(3239, 7)\n",
      "Processing NRCHASH\n",
      "saving (8171, 7)\t(1405, 7)\t(3239, 7)\n",
      "Processing MSOL\n",
      "Final shape (8171, 319) (1405, 319)\n",
      "saving (8171, 319)\t(1405, 319)\t(3239, 319)\n",
      "Processing MPQA\n",
      "saving (8171, 7)\t(1405, 7)\t(3239, 7)\n",
      "Processing DAL\n",
      "saving (8171, 5)\t(1405, 5)\t(3239, 5)\n",
      "Processing BING\n",
      "saving (8171, 7)\t(1405, 7)\t(3239, 7)\n",
      "Processing ANEW\n",
      "Final shape (8171, 181) (1405, 181)\n",
      "saving (8171, 181)\t(1405, 181)\t(3239, 181)\n",
      "Processing SENTN\n",
      "Final shape (8171, 2572) (1405, 2572)\n",
      "saving (8171, 2572)\t(1405, 2572)\t(3239, 2572)\n",
      "Processing EMOLX\n",
      "saving (8171, 50)\t(1405, 50)\t(3239, 50)\n",
      "Processing SENTS\n",
      "saving (8171, 5)\t(1405, 5)\t(3239, 5)\n",
      "Processing LEW\n",
      "saving (8171, 5)\t(1405, 5)\t(3239, 5)\n",
      "Processing EMOSNET\n",
      "saving (8171, 10)\t(1405, 10)\t(3239, 10)\n",
      "Processing SSTREN\n",
      "saving (8171, 1)\t(1405, 1)\t(3239, 1)\n"
     ]
    }
   ],
   "source": [
    "for lex in r.lexs:\n",
    "    print 'Processing', lex.prefix\n",
    "    save_features(lex, train_tokens, labels, dev_tokens, dev_labels, test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving (8171, 8)\t(1405, 8)\t(3239, 8)\n"
     ]
    }
   ],
   "source": [
    "reload(r)\n",
    "save_features(r.swn, train_tokens, labels, dev_tokens, dev_labels, test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
