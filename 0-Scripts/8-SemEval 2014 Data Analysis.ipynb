{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import libs.config\n",
    "import libs.files as fh\n",
    "import libs.pipeline as pipe\n",
    "import libs.parse as p\n",
    "import libs.twokenize as ark\n",
    "import libs.resources as r\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SemEval 2014, Task9, subtask B. There was some error when I downloaded my files from the 2014 test set. Will fix the output file mixing the original file and the file I have downloaded already. It takes too much time to do the re-downloading of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'libs.files' from 'libs/files.pyc'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the train file\n",
    "tw_downloaded = \"../1-Input/2014/SemEval2014-task9-test-B-gold.txt.error\"\n",
    "original_file = \"../1-Input/2014/SemEval2014-task9-test-B-gold-NEED-TWEET-DOWNLOAD.txt\"\n",
    "final_file = \"../1-Input/2014/SemEval2014-task9-test-B-gold.final.txt\"\n",
    "gold_file = \"../7-SemEval/2014/SemEval2014-task9-test-B-gold.txt\"\n",
    "\n",
    "# the task in SEMEVAL\n",
    "file_type = 'B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the problematic\n",
    "problem_data = {}\n",
    "with codecs.open(tw_downloaded, 'r',  encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        data = line.split('\\t')\n",
    "        \n",
    "        if 'T' in data[1]:\n",
    "            # it is a Tweet\n",
    "            text = data[3].replace('\\r', '').replace('\\n', '')\n",
    "            try:\n",
    "                text = text.decode('unicode-escape')\n",
    "                text = unidecode.unidecode(text)\n",
    "            except:\n",
    "                text = text\n",
    "            problem_data['{}-{}'.format(data[0], data[1])] = {'sent': data[2], 'text':text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a final corrected file\n",
    "final_data = []\n",
    "with codecs.open(original_file, 'r',  encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        data = line.split('\\t')\n",
    "        if len(data)>3:\n",
    "            # it is NOT a Tweet\n",
    "            text = data[3].replace('\\r', '').replace('\\n', '')\n",
    "            try:\n",
    "                text = text.decode('unicode-escape')\n",
    "                text = unidecode.unidecode(text)\n",
    "            except:\n",
    "                text = text\n",
    "            info = {'sent': data[2], 'text':text}\n",
    "        else:\n",
    "            info = problem_data['{}-{}'.format(data[0], data[1])]\n",
    "            \n",
    "        data = '\\t'.join(data[:2] + [info['sent'], info['text']])\n",
    "        final_data.append(data)           \n",
    "        \n",
    "with codecs.open(final_file, 'w', 'utf-8') as f:\n",
    "    for line in final_data:\n",
    "        f.write(\"%s\\n\" % line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8986 rows from ../1-Input/2014/SemEval2014-task9-test-B-gold.final.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7909"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the file\n",
    "test_feat, gold, test_tweets = pipe.create_features(final_file, file_type, '')\n",
    "len(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 9683 rows from ../1-Input/trainingData-B.tsv\n",
      "Read 1653 rows from ../1-Input/devData-B.tsv\n"
     ]
    }
   ],
   "source": [
    "# read the information\n",
    "OUTPUT_DIR = '../3-Output/'\n",
    "MODEL_OUT_DIR = '../6-Models'\n",
    "CREATE_TOKENS_FILES = True\n",
    "PROCESS_DIR = '../2-Processed'\n",
    "rnd = 9000\n",
    "\n",
    "# read the config file\n",
    "cfg = fh.read_config_file(\"all.yaml\")\n",
    "\n",
    "# read the train file\n",
    "file_name = \"../1-Input/trainingData-B.tsv\"\n",
    "file_type = 'B'\n",
    "train, labels, train_tweets = pipe.create_features(file_name, file_type, cfg)\n",
    "\n",
    "# read the dev file\n",
    "file_name = \"../1-Input/devData-B.tsv\"\n",
    "file_type = 'B'\n",
    "dev, dev_labels, dev_tweets = pipe.create_features(file_name, file_type, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (9576, 19847)\n",
      "dev shape: (7909, 19847)\n",
      "Final shape (9576, 993) (7909, 993)\n"
     ]
    }
   ],
   "source": [
    "# random seed for all the operations\n",
    "rnd_seed = 9000\n",
    "\n",
    "# join datasets\n",
    "train_final = train_tweets[:]\n",
    "train_final.extend(dev_tweets)\n",
    "\n",
    "# convert the list into and array that can be indexed\n",
    "train_labels = np.concatenate((np.array(labels),np.array(dev_labels)))\n",
    "gold = np.array(gold)\n",
    "\n",
    "# create the cleaned the tweets\n",
    "train_clean, test_clean, vect = pipe.create_count_vec(train_final, test_tweets, tokenizer=pipe.tokenize_clean_raw, stop_words=pipe.stop_words)\n",
    "train_data, test_data, _ = pipe.auto_select_features(pipe.chi2, 5, train_clean, train_labels, test_clean, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using negated tokens\n",
    "train_neg_tokens = [pipe.tokenize_negate_clean_raw(t) for t in train_final]\n",
    "test_neg_tokens = [pipe.tokenize_negate_clean_raw(t) for t in test_tweets]\n",
    "\n",
    "# not using negated tokens\n",
    "train_tokens = [pipe.tokenize_clean_raw(t) for t in train_final]\n",
    "test_tokens = [pipe.tokenize_clean_raw(t) for t in test_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_gene(ind):\n",
    "    \"\"\"\n",
    "    Decode a gene to a information that can be processed.\n",
    "    \"\"\"\n",
    "    return [r.lexs[idx] for idx, present in enumerate(ind) if present]\n",
    "\n",
    "def select_features(lex, train_feat, labels, test_feat):\n",
    "    \"\"\"\n",
    "    Select the most important features according to a given criteria.\n",
    "    \"\"\"\n",
    "    tmp_train_feat, tmp_test_feat, _, _ = pipe.create_lex_vec(train_feat, test_feat)\n",
    "    tmp_train_feat, tmp_test_feat, selector = pipe.auto_select_features(pipe.mutual_info_classif, lex.selection_percent, \n",
    "                                                                       tmp_train_feat, labels, \n",
    "                                                                       tmp_test_feat, None)\n",
    "    return tmp_train_feat, tmp_test_feat\n",
    "\n",
    "def create_ind_features(ind, base_train, base_test, train_tokens, test_tokens):\n",
    "    \"\"\"\n",
    "    Given an individual, create the features according to its genes\n",
    "    Params:\n",
    "        ind: individual generated in the genetic algorithm\n",
    "    Returns:\n",
    "        train and test data as a scipy sparse matrix\n",
    "    \"\"\"\n",
    "     # remove the last locus as it refers to the LIWC\n",
    "    LIWC_gene = ind[-1]\n",
    "    tmp_ind = ind[:-1]\n",
    "    \n",
    "    # load the lexicons according to the gens\n",
    "    ind_lexs = decode_gene(tmp_ind)\n",
    "    \n",
    "    # for each of the lexicons, merge them\n",
    "    final_train = base_train.copy()\n",
    "    final_test = base_test.copy()\n",
    "\n",
    "    for lex in ind_lexs:\n",
    "#         try:\n",
    "        print('Joining the lex {}'.format(lex.prefix))\n",
    "        # create the features from the datasets tokens\n",
    "        if False:\n",
    "            train_feat = lex.process_lex(train_neg_tokens, use_best_features=True)\n",
    "            test_feat = lex.process_lex(test_neg_tokens, use_best_features=True)\n",
    "        else:\n",
    "            train_feat = lex.process_lex(train_tokens, use_best_features=True)\n",
    "            test_feat = lex.process_lex(test_tokens, use_best_features=True)\n",
    "\n",
    "#         print len(train_feat), len(test_feat)\n",
    "#             print  test_feat[:3]\n",
    "\n",
    "        # select the top percent features if this is the case. Set the flag to create vector at save if selection occurs\n",
    "        if lex.selection_percent:\n",
    "            train_feat, test_feat = select_features(lex, train_feat, train_labels, test_feat)\n",
    "        else:\n",
    "            train_feat, test_feat, _,_ = pipe.create_lex_vec(train_feat, test_feat)\n",
    "\n",
    "#         print train_data.shape, test_data.shape\n",
    "        final_train, final_test = pipe.join_lex_features(final_train, train_feat, \n",
    "                                                         final_test, test_feat, \n",
    "                                                         verbose=True, create_vec=False)\n",
    "#         except:\n",
    "#             u.print_exception()\n",
    "#             print 'error loading ind', ind\n",
    "#             print lex\n",
    "\n",
    "        # check if should add LIWC\n",
    "    if LIWC_gene:\n",
    "        final_train, final_test = pipe.join_lex_features(final_train, full_train_liwc, \n",
    "                                                        final_test, test_liwc, \n",
    "                                                        verbose=False, create_vec=True)   \n",
    "        print final_train.shape, final_test.shape\n",
    "    return final_train, final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining the lex BING\n",
      "train data, lex and final shape:  (9576, 993) (9576, 7) (9576, 1000)\n",
      "test data, lex and final shape:  (7909, 993) (7909, 7) (7909, 1000)\n",
      "Joining the lex SWN\n",
      "train data, lex and final shape:  (9576, 1000) (9576, 8) (9576, 1008)\n",
      "test data, lex and final shape:  (7909, 1000) (7909, 8) (7909, 1008)\n",
      "Joining the lex MSOL\n",
      "Final shape (9576, 413) (7909, 413)\n",
      "train data, lex and final shape:  (9576, 1008) (9576, 413) (9576, 1421)\n",
      "test data, lex and final shape:  (7909, 1008) (7909, 413) (7909, 1421)\n",
      "Joining the lex NRCHASH\n",
      "train data, lex and final shape:  (9576, 1421) (9576, 7) (9576, 1428)\n",
      "test data, lex and final shape:  (7909, 1421) (7909, 7) (7909, 1428)\n",
      "Joining the lex TSLEX\n",
      "train data, lex and final shape:  (9576, 1428) (9576, 7) (9576, 1435)\n",
      "test data, lex and final shape:  (7909, 1428) (7909, 7) (7909, 1435)\n",
      "Joining the lex WNA\n",
      "Final shape (9576, 111) (7909, 111)\n",
      "train data, lex and final shape:  (9576, 1435) (9576, 111) (9576, 1546)\n",
      "test data, lex and final shape:  (7909, 1435) (7909, 111) (7909, 1546)\n",
      "Joining the lex DAL\n",
      "train data, lex and final shape:  (9576, 1546) (9576, 5) (9576, 1551)\n",
      "test data, lex and final shape:  (7909, 1546) (7909, 5) (7909, 1551)\n",
      "Joining the lex EMOLX\n",
      "train data, lex and final shape:  (9576, 1551) (9576, 50) (9576, 1601)\n",
      "test data, lex and final shape:  (7909, 1551) (7909, 50) (7909, 1601)\n"
     ]
    }
   ],
   "source": [
    "# read best gene\n",
    "df = pd.read_csv('../3-Output/population.csv', header=None, sep=';')\n",
    "df.columns = ['Gene', 'Stats']\n",
    "\n",
    "# create the best gene combination\n",
    "best_individual = eval(df.iloc[0]['Gene'])\n",
    "X_train, X_test = create_ind_features(best_individual, \n",
    "                                      train_data, test_data, \n",
    "                                      train_neg_tokens, test_neg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_val_scores = pipe.run_multiple_class(X_train, train_labels, X_test, gold, \n",
    "                                           rnd_seed=rnd_seed, use_best_params=True,\n",
    "                                           test_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adrianow/Documents/Programs/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/sag.py:286: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "final_scores = pipe.predict_test_multi_proc('LogisticRegression', X_train, train_labels, X_test, gold, test_name='test', rnd_seed=rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "      <th>f1_test_neg</th>\n",
       "      <th>f1_test_neu</th>\n",
       "      <th>f1_test_pos</th>\n",
       "      <th>train std</th>\n",
       "      <th>test std</th>\n",
       "      <th>f1_test_neg std</th>\n",
       "      <th>f1_test_neu std</th>\n",
       "      <th>f1_test_pos std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.75076</td>\n",
       "      <td>0.67038</td>\n",
       "      <td>0.62608</td>\n",
       "      <td>0.747834</td>\n",
       "      <td>0.714679</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.00031</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    train score  test score  f1_test_neg  f1_test_neu  \\\n",
       "LogisticRegression      0.75076     0.67038      0.62608     0.747834   \n",
       "\n",
       "                    f1_test_pos  train std  test std  f1_test_neg std  \\\n",
       "LogisticRegression     0.714679   0.000136  0.000226          0.00031   \n",
       "\n",
       "                    f1_test_neu std  f1_test_pos std  \n",
       "LogisticRegression         0.000123         0.000191  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the output\n",
    "cls = pipe.LogisticRegression(**pipe.lr_params)\n",
    "cls.set_params(random_state=rnd_seed)\n",
    "cls.fit(X_train, train_labels)\n",
    "pred_test = cls.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decode = {\n",
    "    -1: \"negative\",\n",
    "    0:\"neutral\",\n",
    "    1: \"positive\"\n",
    "}\n",
    "pred_test_labels = [ decode[val] for val in pred_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate final file for submission\n",
    "The final submission has a specific format that needs to be followed so that it can be properly evaluated. As some tweets were not available, the prediction is made as neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dictionary with all the key/answers pair as we have missing text in the output\n",
    "ans_dict = {}\n",
    "for tf,pred in zip(test_feat, pred_test_labels):\n",
    "    ans_dict[u\"{}-{}\".format(tf.sid, tf.uid)] = pred\n",
    "    \n",
    "# create the final submission files\n",
    "out_text = []\n",
    "with codecs.open(original_file, 'r',  encoding='utf-8', errors='ignore') as f:\n",
    "    for i, out in enumerate(f):\n",
    "        data = out.split('\\t')\n",
    "        if len(data)==3:\n",
    "            sid, uid, sent = data\n",
    "        else:\n",
    "            sid, uid, sent, _ = data\n",
    "        text = \"NA\\t{}\\t{}\\n\".format(i+1, ans_dict.get(u\"{}-{}\".format(sid, uid), 'neutral'))\n",
    "        out_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_file = '../3-Output/prediction_2014.csv'\n",
    "with codecs.open(out_file, 'w', 'utf-8') as f:\n",
    "    f.writelines(out_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final submission scored by SemEval 2014 script was :\n",
    "\n",
    "| LiveJournal2014\t| \tSMS2013\t| \tTwitter2013\t| \tTwitter2014\t| \tTwitter2014Sarcasm\t| \n",
    "|-------------------------------|--------|-------|-------|-------|\n",
    "|71.11  | 65.89     | 58.51     | 56.21     |  42.05     |\n",
    "\n",
    "\n",
    "\n",
    "Not a very good score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disconsidering the failed tweets\n",
    "Redoing the calculus, using only the information that was able to be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "orig_data = []\n",
    "decode = {\n",
    "    \"negative\":-1,\n",
    "    \"neutral\":0,\n",
    "    \"positive\":1,\n",
    "    None:None\n",
    "}\n",
    "# get the dataset for each of the tweets\n",
    "with codecs.open(original_file, 'r',  encoding='utf-8', errors='ignore') as f:\n",
    "    with codecs.open(gold_file, 'r',  encoding='utf-8', errors='ignore') as f1:\n",
    "        for orig, gold in zip(f, f1):\n",
    "            data_orig = orig.strip().split('\\t')\n",
    "            data_gold = gold.strip().split('\\t')\n",
    "            if len(data_orig)==3:\n",
    "                sid, uid, sent = data_orig\n",
    "                text = ''\n",
    "            else:\n",
    "                sid, uid, sent, text = data_orig\n",
    "            \n",
    "            orig_data.append([data_gold[1], sid, uid, text, decode[sent], decode[ans_dict.get(\"{}-{}\".format(sid,uid), None)] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataSet</th>\n",
       "      <th>sid</th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>gold_sent</th>\n",
       "      <th>pred_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter2014</td>\n",
       "      <td>249298288367525888</td>\n",
       "      <td>T14114531</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter2013</td>\n",
       "      <td>189963607449141249</td>\n",
       "      <td>T13118198</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter2014</td>\n",
       "      <td>282031301962395648</td>\n",
       "      <td>T14111200</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SMS2013</td>\n",
       "      <td>11975</td>\n",
       "      <td>SM112166</td>\n",
       "      <td>Yar he quite clever but aft many guesses lor. ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LiveJournal2014</td>\n",
       "      <td>136592</td>\n",
       "      <td>LJ112295</td>\n",
       "      <td>Yeah we have Thin Lizzy here I HATE the inform...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           DataSet                 sid        uid  \\\n",
       "0      Twitter2014  249298288367525888  T14114531   \n",
       "1      Twitter2013  189963607449141249  T13118198   \n",
       "2      Twitter2014  282031301962395648  T14111200   \n",
       "3          SMS2013               11975   SM112166   \n",
       "4  LiveJournal2014              136592   LJ112295   \n",
       "\n",
       "                                                text  gold_sent  pred_sent  \n",
       "0                                                             1        NaN  \n",
       "1                                                             0        0.0  \n",
       "2                                                             0        1.0  \n",
       "3  Yar he quite clever but aft many guesses lor. ...         -1        1.0  \n",
       "4  Yeah we have Thin Lizzy here I HATE the inform...         -1       -1.0  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = pd.DataFrame(data = orig_data, columns=['DataSet', 'sid', 'uid', 'text', 'gold_sent', 'pred_sent'])\n",
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LiveJournal2014</th>\n",
       "      <th>SMS2013</th>\n",
       "      <th>Twitter2013</th>\n",
       "      <th>Twitter2014</th>\n",
       "      <th>Twitter2014Sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71.11</td>\n",
       "      <td>65.89</td>\n",
       "      <td>67.12</td>\n",
       "      <td>64.07</td>\n",
       "      <td>42.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LiveJournal2014 SMS2013 Twitter2013 Twitter2014 Twitter2014Sarcasm\n",
       "0           71.11   65.89       67.12       64.07              42.05"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = {}\n",
    "for db in res_df.DataSet.unique():\n",
    "    ds_df = res_df[ (res_df.DataSet==db) & (~res_df.pred_sent.isnull())]\n",
    "    ret[db] = [ '{0:0.2f}'.format(100.0*pipe.score_func(ds_df.gold_sent, ds_df.pred_sent))]\n",
    "pd.DataFrame(ret)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
