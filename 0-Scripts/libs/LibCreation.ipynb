{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import parse as p\n",
    "import files as f\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr = f.TweetReader(\"../../1-Input/tweeter-dev-full-B.tsv\", \"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = tr.__iter__().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentiment': u'neutral',\n",
       " 'sid': u'260097528899452929',\n",
       " 'text': u\"Won the match #getin . Plus, tomorrow is a very busy day, with Awareness Day's and debates. Gulp. Debates...\",\n",
       " 'uid': u'595739778'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tweet(object):\n",
    "    \"\"\"Holds a tweet and the processed tokens\"\"\"\n",
    "    def __init__(self, msg, sent=None, sid=0, uid=0):\n",
    "        super(Tweet, self).__init__()\n",
    "\n",
    "        # original message, cleaned message\n",
    "        self.msg = msg\n",
    "        self.msg_cleaned = None\n",
    "        self.sent = sent\n",
    "        self.sid = sid\n",
    "        self.uid = uid\n",
    "\n",
    "        # processed message and different tokens\n",
    "        self.tokens = None\n",
    "        self.bigrams = None\n",
    "        self.trigrams = None\n",
    "        self.ngrams = None\n",
    "        self.non_contiguous = None\n",
    "        \n",
    "        # additional features\n",
    "        self.all_caps_num = 0\n",
    "        self.mentions_num = 0\n",
    "        self.hash_num = 0\n",
    "        self.pos_num_dic = None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        ret=  \"msg: \" + self.msg + \"\\n\"\n",
    "        if self.tokens:\n",
    "            ret = ret + \"\\n\\ntokens: \" + \"|\".join(self.tokens)\n",
    "        if self.bigrams:\n",
    "            ret = ret + \"\\n\\nbrigrams: \" + \"|\".join(self.bigrams)\n",
    "        if self.trigrams:\n",
    "            ret = ret + \"\\n\\ntrigrams: \" + \"|\".join(self.trigrams)\n",
    "        if self.non_contiguous:\n",
    "            ret = ret + \"\\n\\nnon_contiguous: \" + \"|\".join(self.non_contiguous)\n",
    "        return ret\n",
    "\n",
    "    def process(self, token_type='informal'):\n",
    "        \"\"\" \n",
    "        Creates tokens, brigrams, trigrams\n",
    "        \"\"\"\n",
    "        # use ntlk to make the ngrams\n",
    "        self.tokens = self._tokenize_clean(token_type)\n",
    "        self.bigrams = [ \" \".join(t)  for t in p.bigrams(self.tokens) ]\n",
    "        self.trigrams = [ \" \".join(t)  for t in p.trigrams(self.tokens) ]\n",
    "        self.non_contiguous = [ \" \".join(t)  for t in p.non_contiguous(self.trigrams) ]\n",
    "        \n",
    "    def _tokenize_clean(self, token_type):\n",
    "        \"\"\"\n",
    "        Tokenize and clean the tweet\n",
    "        \"\"\"\n",
    "        tokens = p.tokenize(self.msg, token_type)\n",
    "        \n",
    "        # count the original data\n",
    "        self.all_caps_num = p.count_all_caps(tokens)\n",
    "        self.mentions_num = p.count_mentions(tokens)\n",
    "        self.hash_num = p.count_hash(tokens)\n",
    "        #self.pos_num_dic = p.count_pos(self.msg) \n",
    "        \n",
    "        # normalize data\n",
    "        \n",
    "        tokens = p.normalize_lower(tokens)\n",
    "        tokens = p.normalize_mentions(tokens)\n",
    "        tokens = p.normalize_hash(tokens)\n",
    "        tokens = p.normalize_url(tokens)\n",
    "        \n",
    "        # save the normalized message\n",
    "        self.msg_cleaned = \" \".join(tokens)\n",
    "        \n",
    "        # create negated tokens\n",
    "        \n",
    "        # finally return the data\n",
    "        return tokens\n",
    "    \n",
    "    def get_token_features(self):\n",
    "        \"\"\"\n",
    "        Create a dictionary with all the words marked as true\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        for token in self.tokens:\n",
    "            features[token] = True\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def get_stats_features(self):\n",
    "        \"\"\"\n",
    "        Create a dictionary with the statistical features\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        features[\"STS_ALL_CAPS\"] = self.all_caps_num\n",
    "        features[\"STS_MENTIONS\"] = self.mentions_num\n",
    "        features[\"STS_HASH\"] = self.hash_num\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    def get_all_features(self):\n",
    "        \"\"\"\n",
    "        Creates a single dictionary with all the features\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        features.update(self.get_token_features())\n",
    "        features.update(self.get_stats_features())\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'and': True, u\"day's\": True, u'tomorrow': True, u'...': True, u'very': True, u'is': True, '#HASH': True, u'with': True, u'day': True, 'STS_MENTIONS': 0, 'STS_HASH': 1, u'a': True, u'busy': True, u'debates': True, 'STS_ALL_CAPS': 0, u',': True, u'.': True, u'gulp': True, u'won': True, u'plus': True, u'match': True, u'the': True, u'awareness': True}\n",
      "Won the match #getin . Plus, tomorrow is a very busy day, with Awareness Day's and debates. Gulp. Debates...\n",
      "{'STS_ALL_CAPS': 0, 'STS_MENTIONS': 0, 'STS_HASH': 1}\n"
     ]
    }
   ],
   "source": [
    "tw = Tweet(t[\"text\"])\n",
    "tw.process()\n",
    "print tw.get_all_features()\n",
    "print t[\"text\"]\n",
    "print tw.get_stats_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Won the match #getin . Plus, tomorrow is a very busy day, with Awareness Day's and debates. Gulp. Debates... neutral\n",
      "won the match #HASH . plus , tomorrow is a very busy day , with awareness day's and debates . gulp . debates ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'anew' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4cee8ea0d561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg_cleaned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0manew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mnhash\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'anew' is not defined"
     ]
    }
   ],
   "source": [
    "all_tokens = []\n",
    "for t in tr:\n",
    "    tw =Tweet(t[\"text\"], t[\"sentiment\"], t[\"sid\"], t[\"uid\"])\n",
    "    tw.process()\n",
    "    print tw.msg, tw.sent\n",
    "    print tw.msg_cleaned\n",
    "    print anew.process_tweet(tw)\n",
    "    print nhash.process_tweet(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nhash' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bce36e217fa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnhash\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nhash' is not defined"
     ]
    }
   ],
   "source": [
    "nhash.print_match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anew' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b69d180d899c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'anew' is not defined"
     ]
    }
   ],
   "source": [
    "anew.print_match()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from config import RESOURCES_DIR\n",
    "from os.path import join\n",
    "from utils import prefix_dict\n",
    "\n",
    "class BaseLexicon(object):\n",
    "    \"\"\"\n",
    "    Base class that holds the minimal information needed to properly\n",
    "    deal with a tweet and save stats\n",
    "    \"\"\"\n",
    "    def __init__(self, prefix, bigrams=False, trigrams=False, contiguous=False, radicals=False, opinion=True):\n",
    "        # save what is contained in this lexicon\n",
    "        self.prefix = prefix\n",
    "        self.bigrams = bigrams\n",
    "        self.trigrams = trigrams\n",
    "        self.contiguous = contiguous\n",
    "        self.radicals = radicals\n",
    "        self.opinion = opinion\n",
    "        \n",
    "        # the lexicon info\n",
    "        self.data = None\n",
    "        self.df = None\n",
    "        self.token_num = 0\n",
    "        \n",
    "        # stats info\n",
    "        self.total_words = 0\n",
    "        self.total_tweets = 0\n",
    "        \n",
    "        # general information on processed tokens\n",
    "        self.total_bigrams = 0\n",
    "        self.total_trigrams = 0\n",
    "        self.total_contiguous = 0\n",
    "        self.total_words = 0\n",
    "\n",
    "        # match info\n",
    "        self.total_tweets_match = 0\n",
    "        self.total_words_match = 0\n",
    "        self.total_bigrams_match = 0\n",
    "        self.total_trigrams_match = 0\n",
    "        self.total_contiguous_match = 0\n",
    "        \n",
    "    def _load_lexicon_json(self, file_name):\n",
    "        \"\"\"\n",
    "        loads the preprocessed json\n",
    "        \"\"\"\n",
    "        # construct path\n",
    "        file_name = join(RESOURCES_DIR,\n",
    "                         'processed',\n",
    "                         file_name)\n",
    "        \n",
    "        # load the json\n",
    "        with open(file_name, 'r')  as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        # fill other structures\n",
    "        self.token_num = len(self.data.keys())\n",
    "        self._create_df()\n",
    "        \n",
    "    def _create_df(self):\n",
    "        \"\"\"\n",
    "        Creates a DF from the data. Prefix all names\n",
    "        \"\"\"\n",
    "        self.df = pd.DataFrame.from_dict(self.data, orient='index')\n",
    "        self.df.columns = [ self.prefix+\"_\"+c if c!=0 else self.prefix for c in self.df.columns ]\n",
    "            \n",
    "    def compare_lexicons(self, other):\n",
    "        \"\"\"\n",
    "        Compares this lexicon COVERAGE with another one\n",
    "        \"\"\"\n",
    "        # for now, just a simple difference\n",
    "        data = other.data\n",
    "        same = set(data.keys()).intersection(self.data.keys())\n",
    "        diff = set(data.keys()).difference(self.data.keys())\n",
    "        per_same = (len(same)*100.0)/self.token_num\n",
    "        \n",
    "        # check the rÂ² between metrics\n",
    "        \n",
    "        if False:\n",
    "            print \"{}\\t{}\\t{}\".format(self.prefix, other.prefix, \"Common\")\n",
    "            print \"{}\\t{}\\t{}\\n\".format(self.token_num, \n",
    "                                        len(data.keys()), \n",
    "                                        len(same))\n",
    "        return {self.prefix:{other.prefix: per_same}}\n",
    "    \n",
    "    def correlate_lexicon(self, other):\n",
    "        \"\"\"\n",
    "        Calculates the CORRELATION between 2 OPINION lexicons\n",
    "        \"\"\"\n",
    "        if self.opinion and other.opinion:\n",
    "            temp_df = self.df.join(other.df)\n",
    "            temp_df.dropna(inplace=True)\n",
    "            corr = temp_df.corr()\n",
    "\n",
    "            return {corr.columns[0]: {corr.columns[1]: corr.iloc[0,1]}}\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def process_tweet(self, tweet):\n",
    "        \"\"\"\n",
    "        Process a tweet\n",
    "        Params:\n",
    "            Tweet Class\n",
    "        \"\"\"\n",
    "        # process the basic tokens\n",
    "        sent_list = []\n",
    "        for token in tweet.tokens:\n",
    "            # get the value for this token\n",
    "            ret = self.data.get(token, None)\n",
    "            if ret:\n",
    "                self.total_words_match += 1\n",
    "                sent_list.append({token: ret})\n",
    "        self.total_words += len(tweet.tokens)\n",
    "        \n",
    "        # process bigrams\n",
    "        if self.bigrams:\n",
    "            for token in tweet.bigrams:\n",
    "                # get the value for this token\n",
    "                ret = self.data.get(token, None)\n",
    "                if ret:\n",
    "                    self.total_bigrams_match += 1\n",
    "                    sent_list.append({token: ret})\n",
    "            self.total_bigrams += len(tweet.bigrams)\n",
    "\n",
    "        # process trigrams\n",
    "        if self.trigrams:\n",
    "            for token in tweet.trigrams:\n",
    "                # get the value for this token\n",
    "                ret = self.data.get(token, None)\n",
    "                if ret:\n",
    "                    self.total_trigrams_match += 1\n",
    "                    sent_list.append({token: ret})\n",
    "            self.total_trigrams += len(tweet.trigrams)\n",
    "\n",
    "        # process contiguous\n",
    "        if self.contiguous:\n",
    "            for token in tweet.contiguous:\n",
    "                # get the value for this token\n",
    "                ret = self.data.get(token, None)\n",
    "                if ret:\n",
    "                    self.total_contiguous_match += 1\n",
    "                    sent_list.append({token: ret})\n",
    "            self.total_contiguous += len(tweet.contiguous)\n",
    "\n",
    "        \n",
    "        self.total_tweets +=1\n",
    "        \n",
    "        # check if this tweet had any match\n",
    "        if len(sent_list)>0:\n",
    "            self.total_tweets_match += 1 \n",
    "            \n",
    "            # check if this an opinion lexicon\n",
    "            if self.opinion:\n",
    "                summ = 0\n",
    "                print sent_list\n",
    "                for sent in sent_list:\n",
    "                    summ += sent.values()[0]\n",
    "\n",
    "                return {'SUM_'+self.prefix: summ}\n",
    "            else:\n",
    "                # it is a sentiment lexicon. Sum the sentiments individually\n",
    "                sum_dic = {}\n",
    "                for ws in sent_list:\n",
    "                    # sum the sentiment values of all words\n",
    "                    for w, e in ws.iteritems():\n",
    "                        print w,e\n",
    "                        for k,v in e.iteritems():\n",
    "                            sum_dic[k] = sum_dic.get(k, 0) + v\n",
    "                \n",
    "                # prefix the dictionary keys before returning it\n",
    "                return prefix_dict(sum_dic, self.prefix+'_')\n",
    "        else:\n",
    "            # no term found in the dictionary\n",
    "            return None\n",
    "    \n",
    "    def print_match(self):\n",
    "        \"\"\"\n",
    "        Print the variables that are related with the match\n",
    "        \"\"\"\n",
    "        # print the variables\n",
    "        print \"Total tweets %d\" % self.total_tweets_match\n",
    "        print \"Total words %d\" % self.total_words_match\n",
    "        print \"Total bigrams %d\" % self.total_bigrams_match\n",
    "        print \"Total trigrams %d\" % self.total_trigrams_match\n",
    "        print \"Total contiguous %d\" % self.total_contiguous_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'wna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-42cd7d2889b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_tweet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'wna'"
     ]
    }
   ],
   "source": [
    "for t in tr:\n",
    "    tw = Tweet(t[\"text\"])\n",
    "    tw.process()\n",
    "    print r.  wna.process_tweet(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AnewLexicon(BaseLexicon):\n",
    "    \"\"\"\n",
    "    Holds the information for AnewLexicon\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(AnewLexicon, self).__init__(prefix='ANEW', opinion=False)\n",
    "        self._load_lexicon_json('anew.json')\n",
    "        \n",
    "anew = AnewLexicon()\n",
    "\n",
    "class BingLexicon(BaseLexicon):\n",
    "    \"\"\"\n",
    "    Holds Bing and Liu Lexicon Info\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BingLexicon, self).__init__(prefix='BING')\n",
    "        self._load_lexicon_json('bing.json')\n",
    "bing = BingLexicon()\n",
    "        \n",
    "class DALLexicon(BaseLexicon):\n",
    "    \"\"\"\n",
    "    Holds the information for AnewLexicon\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DALLexicon, self).__init__(prefix='DAL', opinion=False)\n",
    "        self._load_lexicon_json('dal.json')\n",
    "        \n",
    "dal = DALLexicon()\n",
    "\n",
    "class MPQALexicon(BaseLexicon):\n",
    "    \"\"\"\n",
    "    Holds Bing and Liu Lexicon Info\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MPQALexicon, self).__init__(prefix='MPQA')\n",
    "        self._load_lexicon_json('mpqa.json')\n",
    "mpqa = MPQALexicon()\n",
    "        \n",
    "class MSOLLexicon(BaseLexicon):\n",
    "    \"\"\"\n",
    "    Holds Bing and Liu Lexicon Info\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MSOLLexicon, self).__init__(prefix='MSOL')\n",
    "        self._load_lexicon_json('msol.json')\n",
    "msol = MSOLLexicon()\n",
    "\n",
    "class NRCHashLexicon(BaseLexicon):\n",
    "    \"\"\"\n",
    "    Holds Bing and Liu Lexicon Info\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(NRCHashLexicon, self).__init__(prefix='NRCHASH', bigrams=True)\n",
    "        self._load_lexicon_json('nrc_hash.json')\n",
    "nhash = NRCHashLexicon()\n",
    "\n",
    "class Sent140Lexicon(BaseLexicon):\n",
    "    \"\"\"\n",
    "    Holds Bing and Liu Lexicon Info\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Sent140Lexicon, self).__init__(prefix='SENT140', bigrams=True)\n",
    "        self._load_lexicon_json('sent140.json')\n",
    "s140 = Sent140Lexicon()\n",
    "\n",
    "class SentiStrenghtLexicon(BaseLexicon):\n",
    "    \"\"\"\n",
    "    Holds Bing and Liu Lexicon Info\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SentiStrenghtLexicon, self).__init__(prefix='SSTREN')\n",
    "        self._load_lexicon_json('sentstrenght.json')\n",
    "ss = SentiStrenghtLexicon()\n",
    "\n",
    "class TSLexStrengthLexicon(BaseLexicon):\n",
    "    \"\"\"\n",
    "    Holds Bing and Liu Lexicon Info\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TSLexStrengthLexicon, self).__init__(prefix='TSLEX')\n",
    "        self._load_lexicon_json('ts_lex.json')\n",
    "tslex = TSLexStrengthLexicon()\n",
    "\n",
    "class WNALexicon(BaseLexicon):\n",
    "    \"\"\"\n",
    "    Holds Bing and Liu Lexicon Info\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(WNALexicon, self).__init__(prefix='WNA', opinion=False)\n",
    "        self._load_lexicon_json('wna.json')\n",
    "wna = WNALexicon()\n",
    "lexs = [wna, tslex, ss, s140, nhash, msol, mpqa, dal, bing, anew]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SSTREN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSLEX</th>\n",
       "      <td>0.176277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SSTREN\n",
       "TSLEX  0.176277"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correlate(d1, d2):\n",
    "    if d1.opinion and d2.opinion:\n",
    "        temp_df = d1.df.join(d2.df)\n",
    "        temp_df.dropna(inplace=True)\n",
    "        corr = temp_df.corr()\n",
    "        \n",
    "        \n",
    "        return {corr.columns[0]: {corr.columns[1]: corr.iloc[0,1]}}\n",
    "        #keys = list(set(d1.data.keys()).intersection(d2.data.keys()))\n",
    "        #print keys[0]\n",
    "        #print d1.data[keys[0]]\n",
    "        #print d2.data[keys[0]]\n",
    "    else:\n",
    "        return None\n",
    "a =  correlate(ss, tslex)\n",
    "pd.DataFrame(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "cover = {}\n",
    "corr = {}\n",
    "\n",
    "# make the comparison of the lexicons\n",
    "for comb in itertools.permutations(lexs, 2):\n",
    "    # check the common values between them\n",
    "    t = (comb[0].compare_lexicons(comb[1]))\n",
    "    \n",
    "    # insert the returned dictionary as another key in the comparison\n",
    "    temp_dict = cover.get(t.keys()[0], {})\n",
    "    temp_dict.update(t.values()[0])\n",
    "    cover[t.keys()[0]] = temp_dict\n",
    "    \n",
    "    # check how correlated they are\n",
    "    res = comb[0].correlate_lexicon(comb[1])\n",
    "    if res:\n",
    "        # insert the returned correlation into the dict of comparisons\n",
    "        temp_dict = corr.get(res.keys()[0], {})\n",
    "        temp_dict.update(res.values()[0])\n",
    "        corr[t.keys()[0]] = temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ANEW</th>\n",
       "      <th>BING</th>\n",
       "      <th>DAL</th>\n",
       "      <th>MPQA</th>\n",
       "      <th>MSOL</th>\n",
       "      <th>NRCHASH</th>\n",
       "      <th>SENT140</th>\n",
       "      <th>SSTREN</th>\n",
       "      <th>TSLEX</th>\n",
       "      <th>WNA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ANEW</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6.280407</td>\n",
       "      <td>7.159014</td>\n",
       "      <td>7.022721</td>\n",
       "      <td>1.281414</td>\n",
       "      <td>0.140255</td>\n",
       "      <td>0.076874</td>\n",
       "      <td>4.262542</td>\n",
       "      <td>0.324558</td>\n",
       "      <td>9.347181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BING</th>\n",
       "      <td>41.199226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.923999</td>\n",
       "      <td>78.164650</td>\n",
       "      <td>6.553665</td>\n",
       "      <td>0.488317</td>\n",
       "      <td>0.267830</td>\n",
       "      <td>28.253489</td>\n",
       "      <td>1.374041</td>\n",
       "      <td>51.632047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAL</th>\n",
       "      <td>61.218569</td>\n",
       "      <td>20.757777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.135143</td>\n",
       "      <td>7.791885</td>\n",
       "      <td>1.105999</td>\n",
       "      <td>0.608929</td>\n",
       "      <td>13.127122</td>\n",
       "      <td>1.663656</td>\n",
       "      <td>22.997033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MPQA</th>\n",
       "      <td>46.034816</td>\n",
       "      <td>78.107032</td>\n",
       "      <td>21.567519</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.476440</td>\n",
       "      <td>0.528643</td>\n",
       "      <td>0.286680</td>\n",
       "      <td>28.894757</td>\n",
       "      <td>1.270988</td>\n",
       "      <td>57.344214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSOL</th>\n",
       "      <td>94.680851</td>\n",
       "      <td>73.816895</td>\n",
       "      <td>67.326397</td>\n",
       "      <td>84.272647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.041720</td>\n",
       "      <td>1.091646</td>\n",
       "      <td>36.325915</td>\n",
       "      <td>4.197341</td>\n",
       "      <td>78.041543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NRCHASH</th>\n",
       "      <td>92.166344</td>\n",
       "      <td>48.916409</td>\n",
       "      <td>84.992083</td>\n",
       "      <td>52.994984</td>\n",
       "      <td>18.158377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.169051</td>\n",
       "      <td>29.158808</td>\n",
       "      <td>22.049809</td>\n",
       "      <td>43.397626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SENT140</th>\n",
       "      <td>90.715667</td>\n",
       "      <td>48.179272</td>\n",
       "      <td>84.030762</td>\n",
       "      <td>51.608144</td>\n",
       "      <td>17.434555</td>\n",
       "      <td>50.584862</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.498303</td>\n",
       "      <td>25.626462</td>\n",
       "      <td>43.026706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSTREN</th>\n",
       "      <td>10.928433</td>\n",
       "      <td>11.042312</td>\n",
       "      <td>3.935761</td>\n",
       "      <td>11.301269</td>\n",
       "      <td>1.260471</td>\n",
       "      <td>0.113764</td>\n",
       "      <td>0.064089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.341734</td>\n",
       "      <td>7.344214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSLEX</th>\n",
       "      <td>52.998066</td>\n",
       "      <td>34.203155</td>\n",
       "      <td>31.768831</td>\n",
       "      <td>31.661257</td>\n",
       "      <td>9.276178</td>\n",
       "      <td>5.479222</td>\n",
       "      <td>3.546128</td>\n",
       "      <td>21.765372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.602374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WNA</th>\n",
       "      <td>12.185687</td>\n",
       "      <td>10.260946</td>\n",
       "      <td>3.505994</td>\n",
       "      <td>11.404544</td>\n",
       "      <td>1.376963</td>\n",
       "      <td>0.086096</td>\n",
       "      <td>0.047534</td>\n",
       "      <td>3.734440</td>\n",
       "      <td>0.252302</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ANEW       BING        DAL       MPQA       MSOL    NRCHASH  \\\n",
       "ANEW           NaN   6.280407   7.159014   7.022721   1.281414   0.140255   \n",
       "BING     41.199226        NaN  15.923999  78.164650   6.553665   0.488317   \n",
       "DAL      61.218569  20.757777        NaN  28.135143   7.791885   1.105999   \n",
       "MPQA     46.034816  78.107032  21.567519        NaN   7.476440   0.528643   \n",
       "MSOL     94.680851  73.816895  67.326397  84.272647        NaN   2.041720   \n",
       "NRCHASH  92.166344  48.916409  84.992083  52.994984  18.158377        NaN   \n",
       "SENT140  90.715667  48.179272  84.030762  51.608144  17.434555  50.584862   \n",
       "SSTREN   10.928433  11.042312   3.935761  11.301269   1.260471   0.113764   \n",
       "TSLEX    52.998066  34.203155  31.768831  31.661257   9.276178   5.479222   \n",
       "WNA      12.185687  10.260946   3.505994  11.404544   1.376963   0.086096   \n",
       "\n",
       "           SENT140     SSTREN      TSLEX        WNA  \n",
       "ANEW      0.076874   4.262542   0.324558   9.347181  \n",
       "BING      0.267830  28.253489   1.374041  51.632047  \n",
       "DAL       0.608929  13.127122   1.663656  22.997033  \n",
       "MPQA      0.286680  28.894757   1.270988  57.344214  \n",
       "MSOL      1.091646  36.325915   4.197341  78.041543  \n",
       "NRCHASH  28.169051  29.158808  22.049809  43.397626  \n",
       "SENT140        NaN  29.498303  25.626462  43.026706  \n",
       "SSTREN    0.064089        NaN   0.341734   7.344214  \n",
       "TSLEX     3.546128  21.765372        NaN  31.602374  \n",
       "WNA       0.047534   3.734440   0.252302        NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
